{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r5/vlpl089n6ng23s67pjdhzyt00000gn/T/ipykernel_93052/1893612302.py:20: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey, Float\n",
    "from sqlalchemy.orm import sessionmaker, relationship, declarative_base\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "engine = create_engine(conn_str)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "from sqlalchemy import Column, String, Float, Integer, Date, Time, ForeignKey\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Definition of the InstrumentData table\n",
    "class InstrumentData(Base):\n",
    "    __tablename__ = \"instrumentData\"\n",
    "\n",
    "    esmId = Column(String(20), primary_key=True, index=True)\n",
    "    isin = Column(String(30), index=True)\n",
    "    ticker = Column(String(20))\n",
    "    coupon = Column(Float)\n",
    "    maturity = Column(Date)\n",
    "    sector = Column(String(30))\n",
    "    rating = Column(String(10))\n",
    "    product = Column(String(30))\n",
    "    country = Column(String(20))\n",
    "    esgScore = Column(Integer)\n",
    "\n",
    "    trades = relationship(\"TradeData\", back_populates=\"instrument\")\n",
    "    esg_data = relationship(\"EsgData\", back_populates=\"instrument\")\n",
    "\n",
    "# Definition of the TradeData table\n",
    "class TradeData(Base):\n",
    "    __tablename__ = \"tradeData\"\n",
    "\n",
    "    srNo = Column(Integer, primary_key=True, index=True)\n",
    "    esmId = Column(String(20), ForeignKey(\"instrumentData.esmId\"))\n",
    "    tradeId = Column(String(20), unique=True, index=True)\n",
    "    client = Column(String(20))\n",
    "    notional = Column(Integer)\n",
    "    quote = Column(Integer)\n",
    "    price = Column(Float)\n",
    "    traderName = Column(String(30))\n",
    "    book = Column(String(20))\n",
    "    position = Column(Integer)\n",
    "    barclaysSide = Column(String(10))\n",
    "    tradeDate = Column(Date)\n",
    "\n",
    "    instrument = relationship(\"InstrumentData\", back_populates=\"trades\")\n",
    "\n",
    "# Definition of the EsgData table\n",
    "class EsgData(Base):\n",
    "    __tablename__ = \"esgData\"\n",
    "\n",
    "    esmId = Column(String(20), ForeignKey(\"instrumentData.esmId\"), primary_key=True, index=True)\n",
    "    overallScore = Column(Integer)\n",
    "    environmental = Column(String(30))\n",
    "    social = Column(String(30))\n",
    "    governance = Column(String(30))\n",
    "    comment = Column(String(5000))\n",
    "\n",
    "    instrument = relationship(\"InstrumentData\", back_populates=\"esg_data\")\n",
    "\n",
    "# Definition of the TraceData table\n",
    "class TraceData(Base):\n",
    "    __tablename__ = \"traceData\"\n",
    "\n",
    "    isin = Column(String(30), primary_key=True, index=True)\n",
    "    ticker = Column(String(20))\n",
    "    coupon = Column(Float)\n",
    "    maturity = Column(Date)\n",
    "    quote = Column(Integer)\n",
    "    notional = Column(Integer)\n",
    "    date = Column(Date)\n",
    "    time = Column(Time)\n",
    "    market_entry = relationship(\"MarketData\", back_populates=\"traces\")\n",
    "\n",
    "# Definition of the MarketData table\n",
    "class MarketData(Base):\n",
    "    __tablename__ = \"marketData\"\n",
    "\n",
    "    isin = Column(String(30), primary_key=True, index=True)\n",
    "    ticker = Column(String(20))\n",
    "    coupon = Column(Float)\n",
    "    maturity = Column(Date)\n",
    "    marketPrice = Column(Float)\n",
    "    date = Column(Date)\n",
    "    time = Column(Time)\n",
    "    traces = relationship(\"TraceData\", back_populates=\"market_entry\")\n",
    "\n",
    "# Definition of the ClientData table\n",
    "class ClientData(Base):\n",
    "    __tablename__ = \"clientData\"\n",
    "\n",
    "    srno = Column(Integer, primary_key=True, index=True)\n",
    "    client = Column(String(30), unique=True, index=True)\n",
    "    restricted = Column(String(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_agent.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sqlalchemy import text, inspect\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    sql_query: str\n",
    "    query_result: str\n",
    "    query_rows: list\n",
    "    attempts: int\n",
    "    sql_error: bool\n",
    "\n",
    "def get_database_schema(engine):\n",
    "    inspector = inspect(engine)\n",
    "    schema = \"\"\n",
    "    for table_name in inspector.get_table_names():\n",
    "        schema += f\"Table: {table_name}\\n\"\n",
    "        for column in inspector.get_columns(table_name):\n",
    "            col_name = column[\"name\"]\n",
    "            col_type = str(column[\"type\"])\n",
    "            if column.get(\"primary_key\"):\n",
    "                col_type += \", Primary Key\"\n",
    "            if column.get(\"foreign_keys\"):\n",
    "                fk = list(column[\"foreign_keys\"])[0]\n",
    "                col_type += f\", Foreign Key to {fk.column.table.name}.{fk.column.name}\"\n",
    "            schema += f\"- {col_name}: {col_type}\\n\"\n",
    "        schema += \"\\n\"\n",
    "    print(\"Retrieved database schema.\")\n",
    "    return schema\n",
    "\n",
    "# class GetCurrentUser(BaseModel):\n",
    "#     current_user: str = Field(\n",
    "#         description=\"The name of the current user based on the provided user ID.\"\n",
    "#     )\n",
    "\n",
    "# def get_current_user(state: AgentState, config: RunnableConfig):\n",
    "#     print(\"Retrieving the current user based on user ID.\")\n",
    "#     user_id = config[\"configurable\"].get(\"current_user_id\", None)\n",
    "#     if not user_id:\n",
    "#         state[\"current_user\"] = \"User not found\"\n",
    "#         print(\"No user ID provided in the configuration.\")\n",
    "#         return state\n",
    "\n",
    "#     session = SessionLocal()\n",
    "#     try:\n",
    "#         user = session.query(User).filter(User.id == int(user_id)).first()\n",
    "#         if user:\n",
    "#             state[\"current_user\"] = user.name\n",
    "#             print(f\"Current user set to: {state['current_user']}\")\n",
    "#         else:\n",
    "#             state[\"current_user\"] = \"User not found\"\n",
    "#             print(\"User not found in the database.\")\n",
    "#     except Exception as e:\n",
    "#         state[\"current_user\"] = \"Error retrieving user\"\n",
    "#         print(f\"Error retrieving user: {str(e)}\")\n",
    "#     finally:\n",
    "#         session.close()\n",
    "#     return state\n",
    "\n",
    "# class CheckRelevance(BaseModel):\n",
    "#     relevance: str = Field(\n",
    "#         description=\"Indicates whether the question is related to the database schema. 'relevant' or 'not_relevant'.\"\n",
    "#     )\n",
    "\n",
    "# def check_relevance(state: AgentState, config: RunnableConfig):\n",
    "#     question = state[\"question\"]\n",
    "#     schema = get_database_schema(engine)\n",
    "#     print(f\"Checking relevance of the question: {question}\")\n",
    "#     system = \"\"\"You are an assistant that determines whether a given question is related to the following database schema. \\n\n",
    "#     I don't want code as an output, I just want you to tell me if the question is relevant to schema or not.\n",
    "\n",
    "# Schema:\\n\n",
    "# {schema}\n",
    "\n",
    "# Respond with only \"relevant\" or \"not_relevant\".\\n\n",
    "# \"\"\".format(schema=schema)\n",
    "#     human = f\"Question: {question}\"\n",
    "#     check_prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system),\n",
    "#             (\"human\", human),\n",
    "#         ]\n",
    "#     )\n",
    "#     llm = AzureChatOpenAI(azure_deployment=\"gpt-4\", api_version=\"2024-02-01\", temperature=0)\n",
    "#     structured_llm = llm.with_structured_output(CheckRelevance)\n",
    "#     relevance_checker = check_prompt | structured_llm\n",
    "#     relevance = relevance_checker.invoke({})\n",
    "#     state[\"relevance\"] = relevance.relevance\n",
    "#     print(f\"Relevance determined: {state['relevance']}\")\n",
    "#     return state\n",
    "\n",
    "class ConvertToSQL(BaseModel):\n",
    "    sql_query: str = Field(\n",
    "        description=\"The SQL query corresponding to the user's natural language question.\"\n",
    "    )\n",
    "\n",
    "def convert_nl_to_sql(state: AgentState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = get_database_schema(engine)\n",
    "    print(f\"Converting question to SQL for user {question}\")\n",
    "    system = \"\"\"You are an assistant that converts natural language questions into SQL queries based on the following schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Provide only the SQL query without any explanations. Alias columns appropriately to match the expected keys in the result.\n",
    "\n",
    "For example, alias 'food.name' as 'food_name' and 'food.price' as 'price'.\n",
    "\"\"\".format(schema=schema)\n",
    "    convert_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = AzureChatOpenAI(azure_deployment=\"gpt-4-32k\", api_version=\"2024-02-01\", temperature=0)\n",
    "    structured_llm = llm.with_structured_output(ConvertToSQL)\n",
    "    sql_generator = convert_prompt | structured_llm\n",
    "    result = sql_generator.invoke({\"question\": question})\n",
    "    state[\"sql_query\"] = result.sql_query\n",
    "    print(f\"Generated SQL query: {state['sql_query']}\")\n",
    "    return state\n",
    "\n",
    "def execute_sql(state: AgentState):\n",
    "    sql_query = state[\"sql_query\"].strip()\n",
    "    session = SessionLocal()\n",
    "    print(f\"Executing SQL query: {sql_query}\")\n",
    "    try:\n",
    "        result = session.execute(text(sql_query))\n",
    "        if sql_query.lower().startswith(\"select\"):\n",
    "            rows = result.fetchall()\n",
    "            columns = result.keys()\n",
    "            if rows:\n",
    "                # Creating a list of dictionaries mapping column names to their values\n",
    "                state[\"query_rows\"] = [dict(zip(columns, row)) for row in rows]\n",
    "                print(f\"Raw SQL Query Result: {state['query_rows']}\")\n",
    "                # Formatting the result to display each row without assuming specific column names\n",
    "                formatted_result = \"\\n\".join(\", \".join(f\"{col}: {value}\" for col, value in row.items()) for row in state[\"query_rows\"])\n",
    "            else:\n",
    "                state[\"query_rows\"] = []\n",
    "                formatted_result = \"No results found.\"\n",
    "            state[\"query_result\"] = formatted_result\n",
    "            state[\"sql_error\"] = False\n",
    "            print(\"SQL SELECT query executed successfully.\")\n",
    "        else:\n",
    "            session.commit()\n",
    "            state[\"query_result\"] = \"The action has been successfully completed.\"\n",
    "            state[\"sql_error\"] = False\n",
    "            print(\"SQL command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        state[\"query_result\"] = f\"Error executing SQL query: {str(e)}\"\n",
    "        state[\"sql_error\"] = True\n",
    "        print(f\"Error executing SQL query: {str(e)}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "    return state\n",
    "\n",
    "# def generate_human_readable_answer(state: AgentState):\n",
    "#     sql = state[\"sql_query\"]\n",
    "#     result = state[\"query_result\"]\n",
    "#     current_user = state[\"current_user\"]\n",
    "#     query_rows = state.get(\"query_rows\", [])\n",
    "#     sql_error = state.get(\"sql_error\", False)\n",
    "#     print(\"Generating a human-readable answer.\")\n",
    "#     system = \"\"\"You are an assistant that converts SQL query results into clear, natural language responses without including any identifiers like order IDs. Start the response with a friendly greeting that includes the user's name.\n",
    "#     \"\"\"\n",
    "#     if sql_error:\n",
    "#         # Directly relay the error message\n",
    "#         generate_prompt = ChatPromptTemplate.from_messages(\n",
    "#             [\n",
    "#                 (\"system\", system),\n",
    "#                 (\n",
    "#                     \"human\",\n",
    "#                     f\"\"\"SQL Query:\n",
    "# {sql}\n",
    "\n",
    "# Result:\n",
    "# {result}\n",
    "\n",
    "# Formulate a clear and understandable error message in a single sentence, starting with 'Hello {current_user},' informing them about the issue.\"\"\"\n",
    "#                 ),\n",
    "#             ]\n",
    "#         )\n",
    "#     elif sql.lower().startswith(\"select\"):\n",
    "#         if not query_rows:\n",
    "#             # Handle cases with no orders\n",
    "#             generate_prompt = ChatPromptTemplate.from_messages(\n",
    "#                 [\n",
    "#                     (\"system\", system),\n",
    "#                     (\n",
    "#                         \"human\",\n",
    "#                         f\"\"\"SQL Query:\n",
    "# {sql}\n",
    "\n",
    "# Result:\n",
    "# {result}\n",
    "\n",
    "# Formulate a clear and understandable answer to the original question in a single sentence, starting with 'Hello {current_user},' and mention that there are no orders found.\"\"\"\n",
    "#                     ),\n",
    "#                 ]\n",
    "#             )\n",
    "#         else:\n",
    "#             # Handle displaying orders\n",
    "#             generate_prompt = ChatPromptTemplate.from_messages(\n",
    "#                 [\n",
    "#                     (\"system\", system),\n",
    "#                     (\n",
    "#                         \"human\",\n",
    "#                         f\"\"\"SQL Query:\n",
    "# {sql}\n",
    "\n",
    "# Result:\n",
    "# {result}\n",
    "\n",
    "# Formulate a clear and understandable answer to the original question in a single sentence, starting with 'Hello {current_user},' and list each item ordered along with its price. For example: 'Hello Bob, you have ordered Lasagne for $14.0 and Spaghetti Carbonara for $15.0.'\"\"\"\n",
    "#                     ),\n",
    "#                 ]\n",
    "#             )\n",
    "#     else:\n",
    "#         # Handle non-select queries\n",
    "#         generate_prompt = ChatPromptTemplate.from_messages(\n",
    "#             [\n",
    "#                 (\"system\", system),\n",
    "#                 (\n",
    "#                     \"human\",\n",
    "#                     f\"\"\"SQL Query:\n",
    "# {sql}\n",
    "\n",
    "# Result:\n",
    "# {result}\n",
    "\n",
    "# Formulate a clear and understandable confirmation message in a single sentence, starting with 'Hello {current_user},' confirming that your request has been successfully processed.\"\"\"\n",
    "#                 ),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#     llm = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\", api_version=\"2024-02-01\", temperature=0)\n",
    "#     human_response = generate_prompt | llm | StrOutputParser()\n",
    "#     answer = human_response.invoke({})\n",
    "#     state[\"query_result\"] = answer\n",
    "#     print(\"Generated human-readable answer.\")\n",
    "#     return state\n",
    "\n",
    "class RewrittenQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The rewritten question.\")\n",
    "\n",
    "def regenerate_query(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    print(\"Regenerating the SQL query by rewriting the question.\")\n",
    "    system = \"\"\"You are an assistant that reformulates an original question to enable more precise SQL queries. Ensure that all necessary details, such as table joins, are preserved to retrieve complete and accurate data.\n",
    "    \"\"\"\n",
    "    rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"Original Question: {question}\\nReformulate the question to enable more precise SQL queries, ensuring all necessary details are preserved.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    llm = AzureChatOpenAI(azure_deployment=\"gpt-\", api_version=\"2024-02-01\", temperature=0)\n",
    "    structured_llm = llm.with_structured_output(RewrittenQuestion)\n",
    "    rewriter = rewrite_prompt | structured_llm\n",
    "    rewritten = rewriter.invoke({})\n",
    "    state[\"question\"] = rewritten.question\n",
    "    state[\"attempts\"] += 1\n",
    "    print(f\"Rewritten question: {state['question']}\")\n",
    "    return state\n",
    "\n",
    "def generate_funny_response(state: AgentState):\n",
    "    print(\"Generating a funny response for an unrelated question.\")\n",
    "    system = \"\"\"You are a charming and funny assistant who responds in a playful manner.\n",
    "    \"\"\"\n",
    "    human_message = \"I can not help with that, but doesn't asking questions make you hungry? You can always order something delicious.\"\n",
    "    funny_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human_message),\n",
    "        ]\n",
    "    )\n",
    "    llm = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\", api_version=\"2024-02-01\", temperature=0)\n",
    "    funny_response = funny_prompt | llm | StrOutputParser()\n",
    "    message = funny_response.invoke({})\n",
    "    state[\"query_result\"] = message\n",
    "    print(\"Generated funny response.\")\n",
    "    return state\n",
    "\n",
    "def end_max_iterations(state: AgentState):\n",
    "    state[\"query_result\"] = \"Please try again.\"\n",
    "    print(\"Maximum attempts reached. Ending the workflow.\")\n",
    "    return state\n",
    "\n",
    "def relevance_router(state: AgentState):\n",
    "    if state[\"relevance\"].lower() == \"relevant\":\n",
    "        return \"convert_to_sql\"\n",
    "    else:\n",
    "        return \"generate_funny_response\"\n",
    "\n",
    "def check_attempts_router(state: AgentState):\n",
    "    if state[\"attempts\"] < 3:\n",
    "        return \"convert_to_sql\"\n",
    "    else:\n",
    "        return \"end_max_iterations\"\n",
    "\n",
    "def execute_sql_router(state: AgentState):\n",
    "    if not state.get(\"sql_error\", False):\n",
    "        return \"return_context\"\n",
    "    else:\n",
    "        return \"regenerate_query\"\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# workflow.add_node(\"get_current_user\", get_current_user)\n",
    "# workflow.add_node(\"check_relevance\", check_relevance)\n",
    "workflow.add_node(\"convert_to_sql\", convert_nl_to_sql)\n",
    "workflow.add_node(\"execute_sql\", execute_sql)\n",
    "# workflow.add_node(\"generate_human_readable_answer\", generate_human_readable_answer)\n",
    "workflow.add_node(\"regenerate_query\", regenerate_query)\n",
    "# workflow.add_node(\"generate_funny_response\", generate_funny_response)\n",
    "workflow.add_node(\"end_max_iterations\", end_max_iterations)\n",
    "\n",
    "workflow.add_edge(\"convert_to_sql\", \"execute_sql\")\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"check_relevance\",\n",
    "#     relevance_router,\n",
    "#     {\n",
    "#         \"convert_to_sql\": \"convert_to_sql\",\n",
    "#         \"generate_funny_response\": \"generate_funny_response\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# workflow.add_edge(\"convert_to_sql\", \"execute_sql\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"execute_sql\",\n",
    "    execute_sql_router,\n",
    "    {\n",
    "        \"return_context\": END,\n",
    "        \"regenerate_query\": \"regenerate_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"regenerate_query\",\n",
    "    check_attempts_router,\n",
    "    {\n",
    "        \"convert_to_sql\": \"convert_to_sql\",\n",
    "        \"max_iterations\": \"end_max_iterations\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"end_max_iterations\", END)\n",
    "\n",
    "workflow.set_entry_point(\"convert_to_sql\")\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_config = {\"configurable\": {\"current_user_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved database schema.\n",
      "Converting question to SQL for user what is the book exposure to technology sector for trader Tirth\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_question_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the book exposure to technology sector for trader Tirth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result_1 \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_question_1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattempts\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result_1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_result\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1586\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1588\u001b[0m     config,\n\u001b[1;32m   1589\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1590\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1591\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1592\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1593\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1595\u001b[0m ):\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1597\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1315\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1310\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1311\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1312\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1313\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1314\u001b[0m     ):\n\u001b[0;32m-> 1315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1316\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1317\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1318\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1319\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1320\u001b[0m         ):\n\u001b[1;32m   1321\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     run_with_retry(t, retry_policy)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[64], line 126\u001b[0m, in \u001b[0;36mconvert_nl_to_sql\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    124\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(ConvertToSQL)\n\u001b[1;32m    125\u001b[0m sql_generator \u001b[38;5;241m=\u001b[39m convert_prompt \u001b[38;5;241m|\u001b[39m structured_llm\n\u001b[0;32m--> 126\u001b[0m result \u001b[38;5;241m=\u001b[39m sql_generator\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[1;32m    127\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_query\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msql_query\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated SQL query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql_query\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   5355\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5356\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5358\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:705\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    817\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    818\u001b[0m             {\n\u001b[1;32m    819\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    820\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    821\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    822\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    823\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    824\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    825\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    826\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    827\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    828\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    829\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    830\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    831\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    832\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    833\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    834\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    835\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    836\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    837\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    838\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    839\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    842\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    843\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    846\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    847\u001b[0m             },\n\u001b[1;32m    848\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    849\u001b[0m         ),\n\u001b[1;32m    850\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    851\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    852\u001b[0m         ),\n\u001b[1;32m    853\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    854\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    955\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    956\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    957\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    958\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    959\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    960\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1044\u001b[0m         input_options,\n\u001b[1;32m   1045\u001b[0m         cast_to,\n\u001b[1;32m   1046\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1047\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1048\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1049\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1050\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1093\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1094\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1095\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1096\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1097\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1098\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1044\u001b[0m         input_options,\n\u001b[1;32m   1045\u001b[0m         cast_to,\n\u001b[1;32m   1046\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1047\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1048\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1049\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1050\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1093\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1094\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1095\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1096\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1097\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1098\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragv1/lib/python3.12/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"
     ]
    }
   ],
   "source": [
    "user_question_1 = \"what is the book exposure to technology sector for trader Tirth\"\n",
    "result_1 = app.invoke({\"question\": user_question_1, \"attempts\": 0})\n",
    "print(\"Result:\", result_1[\"query_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'order_id': 3, 'food_name': 'Lasagne', 'price': 14.0},\n",
       " {'order_id': 4, 'food_name': 'Spaghetti Carbonara', 'price': 15.0}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_orders_for_user(user_id: int):\n",
    "    session = SessionLocal()\n",
    "    try:\n",
    "        orders = (\n",
    "            session.query(Order)\n",
    "            .join(Food, Order.food_id == Food.id)\n",
    "            .filter(Order.user_id == user_id)\n",
    "            .all()\n",
    "        )\n",
    "        if not orders:\n",
    "            print(f\"No orders found for user_id {user_id}.\")\n",
    "            return []\n",
    "\n",
    "        order_list = []\n",
    "        for order in orders:\n",
    "            order_info = {\n",
    "                \"order_id\": order.id,\n",
    "                \"food_name\": order.food.name,\n",
    "                \"price\": order.food.price\n",
    "            }\n",
    "            order_list.append(order_info)\n",
    "\n",
    "        return order_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving orders for user_id {user_id}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "orders = get_orders_for_user(2)\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving the current user based on user ID.\n",
      "Current user set to: Bob\n",
      "Retrieved database schema.\n",
      "Checking relevance of the question: Tell me a joke.\n",
      "Relevance determined: not_relevant\n",
      "Generating a funny response for an unrelated question.\n",
      "Generated funny response.\n",
      "Result: Well, I am an AI language model, so I don't have a physical body to feel hunger. But I appreciate your concern! Maybe you could tell me about your favorite food instead?\n"
     ]
    }
   ],
   "source": [
    "user_question_2 = \"Tell me a joke.\"\n",
    "result_2 = app.invoke({\"question\": user_question_2, \"attempts\": 0}, config=fake_config)\n",
    "print(\"Result:\", result_2[\"query_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving the current user based on user ID.\n",
      "Current user set to: Bob\n",
      "Retrieved database schema.\n",
      "Checking relevance of the question: Show me my orders\n",
      "Relevance determined: relevant\n",
      "Retrieved database schema.\n",
      "Converting question to SQL for user 'Bob': Show me my orders\n",
      "Generated SQL query: SELECT food.name AS food_name, food.price AS price FROM orders JOIN food ON orders.food_id = food.id JOIN users ON orders.user_id = users.id WHERE users.name = 'Bob'\n",
      "Executing SQL query: SELECT food.name AS food_name, food.price AS price FROM orders JOIN food ON orders.food_id = food.id JOIN users ON orders.user_id = users.id WHERE users.name = 'Bob'\n",
      "Raw SQL Query Result: [{'food_name': 'Lasagne', 'price': 14.0}, {'food_name': 'Spaghetti Carbonara', 'price': 15.0}]\n",
      "SQL SELECT query executed successfully.\n",
      "Generating a human-readable answer.\n",
      "Generated human-readable answer.\n",
      "Result: Hello Bob, you have ordered Lasagne for $14.0 and Spaghetti Carbonara for $15.0.\n"
     ]
    }
   ],
   "source": [
    "user_question_3 = \"Show me my orders\"\n",
    "result_3 = app.invoke({\"question\": user_question_3, \"attempts\": 0}, config=fake_config)\n",
    "print(\"Result:\", result_3[\"query_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved database schema.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Table: clientData\\n- srno: INTEGER\\n- client: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- restricted: VARCHAR(10) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n\\nTable: esgData\\n- esmId: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- overallScore: INTEGER\\n- environmental: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- social: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- governance: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- comment: VARCHAR(5000) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n\\nTable: instrumentData\\n- esmId: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- isin: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- ticker: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- coupon: FLOAT\\n- maturity: DATE\\n- sector: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- rating: VARCHAR(10) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- product: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- country: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- esgScore: INTEGER\\n\\nTable: marketData\\n- isin: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- ticker: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- coupon: FLOAT\\n- maturity: DATE\\n- marketPrice: FLOAT\\n- date: DATE\\n- time: TIME\\n\\nTable: traceData\\n- isin: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- ticker: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- coupon: FLOAT\\n- maturity: DATE\\n- quote: INTEGER\\n- notional: INTEGER\\n- date: DATE\\n- time: TIME\\n\\nTable: tradeData\\n- srNo: INTEGER\\n- esmId: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- tradeId: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- client: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- notional: INTEGER\\n- quote: INTEGER\\n- price: FLOAT\\n- traderName: VARCHAR(30) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- book: VARCHAR(20) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- position: INTEGER\\n- barclaysSide: VARCHAR(10) COLLATE \"SQL_Latin1_General_CP1_CI_AS\"\\n- tradeDate: DATE\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_database_schema(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
